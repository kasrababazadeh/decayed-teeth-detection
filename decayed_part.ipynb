{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Av5OBWsb2Rqw",
        "outputId": "8ecaed0f-4e51-49fd-c27a-717cf2eda8af"
      },
      "outputs": [],
      "source": [
        "# if you are using .DCM files\n",
        "!pip install pydicom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "AUGMENT YOUR DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulIpr9cn5K-E",
        "outputId": "cb48ffc5-10ff-409b-f3b4-fdba1c2cfb21"
      },
      "outputs": [],
      "source": [
        "import pydicom\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "def augment(dicom_path, output_path):\n",
        "  dicom_data = pydicom.dcmread(dicom_path)\n",
        "  original_filename, extension = os.path.splitext(os.path.basename(dicom_path))\n",
        "  image = dicom_data.pixel_array\n",
        "  image = np.expand_dims(image, axis=-1)\n",
        "  datagen = ImageDataGenerator(\n",
        "      vertical_flip=True\n",
        "      # horizontal_flip=True\n",
        "  )\n",
        "  augmented_image = datagen.apply_transform(image, transform_parameters={\n",
        "            # \"flip_horizontal\": True\n",
        "            \"flip_vertical\": True\n",
        "        })\n",
        "  augmented_image = np.squeeze(augmented_image, axis=-1)\n",
        "  dicom_data.PixelData = augmented_image.tobytes()\n",
        "  new_filename = original_filename + \"_v\" + extension\n",
        "  new_dicom_file_path = os.path.join(output_path, new_filename)\n",
        "  dicom_data.save_as(new_dicom_file_path)\n",
        "  print(f\"Saved augmented image as {output_path}\")\n",
        "input_directory = \"/path/to/your_data\"\n",
        "output_directory = \"/path/to/augmented_data\"\n",
        "for filename in os.listdir(input_directory):\n",
        "    if filename.endswith(\".DCM\"):\n",
        "      dicom_path = os.path.join(input_directory, filename)\n",
        "      augment(dicom_path, output_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ORGANIZE YOUR DATA FOLDER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGRaC5qP1EYq",
        "outputId": "3bd01d97-defa-4a16-aaa9-33d003791484"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pydicom\n",
        "source_directory = '/content/drive/MyDrive/decayed_part/premolar'\n",
        "output_directory = '/content/drive/MyDrive/decayed_part/premolar'\n",
        "dicom_files = [os.path.join(source_directory, filename) for filename in os.listdir(source_directory) if filename.endswith('.DCM')]\n",
        "c = 0\n",
        "for dicom_file in dicom_files:\n",
        "  ds = pydicom.dcmread(dicom_file)\n",
        "  # c += 1\n",
        "  file_name = os.path.splitext(os.path.basename(dicom_file))[0]\n",
        "  new_name = file_name + '.DCM'\n",
        "  new_filename = os.path.join(output_directory, new_name)\n",
        "  ds.save_as(new_filename)\n",
        "# print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "APPLYING ROTATION AUGMENTATION AND FILTERS TO IMAGES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpKuWluBPG2i",
        "outputId": "b886cbe4-58e7-4977-bdae-f68709aea4c9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pydicom\n",
        "from PIL import Image\n",
        "\n",
        "input_folder_path = '/path/to/data'\n",
        "output_folder_path = '/path/to/filtered_rotated_data'\n",
        "\n",
        "def rotate_and_save_dicom(input_path, output_path, angle):\n",
        "  dicom_data = pydicom.dcmread(input_path)\n",
        "  filename = os.path.basename(input_path)\n",
        "  ribbon_width = int(0.3 * dicom_data.pixel_array.shape[1])\n",
        "  ribbon_position = dicom_data.pixel_array.shape[1] // 2 - ribbon_width // 2\n",
        "  dicom_data.pixel_array[:, ribbon_position:ribbon_position + ribbon_width] = 0\n",
        "  height, width = dicom_data.pixel_array.shape\n",
        "  black_region_height = int(0.3 * height)\n",
        "  if 'vertical' in filename or 'vertical' in filename:\n",
        "    dicom_data.pixel_array[:black_region_height, :] = 0\n",
        "  else:\n",
        "    dicom_data.pixel_array[-black_region_height:, :] = 0\n",
        "  pixel_array = dicom_data.pixel_array.astype(np.float32)\n",
        "  pixel_array = pixel_array.astype(np.uint8)\n",
        "  image = Image.fromarray(pixel_array)\n",
        "  rotated_image = image.rotate(angle, resample=Image.BICUBIC, expand=True)\n",
        "  rotated_pixel_array = np.array(rotated_image)\n",
        "  dicom_data.PixelData = rotated_pixel_array.tobytes()\n",
        "  dicom_data.Rows, dicom_data.Columns = rotated_pixel_array.shape\n",
        "  dicom_data.save_as(output_path)\n",
        "for filename in os.listdir(input_folder_path):\n",
        "  if filename.endswith('.DCM'):\n",
        "    input_file_path = os.path.join(input_folder_path, filename)\n",
        "    output_file_path_20 = os.path.join(output_folder_path, f'rotated_p20_{filename}')\n",
        "    rotate_and_save_dicom(input_file_path, output_file_path_20, 20)\n",
        "    output_file_path_minus_20 = os.path.join(output_folder_path, f'rotated_m20_{filename}')\n",
        "    rotate_and_save_dicom(input_file_path, output_file_path_minus_20, -20)\n",
        "\n",
        "print(\"Rotation and saving completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LOAD DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o9D3tjM5Yja",
        "outputId": "79ee86d7-c5fd-43c7-ccb6-a9792c495b61"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pydicom\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications.resnet import preprocess_input, decode_predictions\n",
        "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from skimage import exposure\n",
        "path = \"/path/to/data\"\n",
        "images = []\n",
        "labels = []\n",
        "for filename in os.listdir(path):\n",
        "    ds = pydicom.read_file(os.path.join(path, filename))\n",
        "    if 'decayed' in filename:\n",
        "      labels.append(1)\n",
        "    elif 'non_decayed' in filename:\n",
        "      labels.append(2)\n",
        "    pixel_array = ds.pixel_array.astype(np.uint8)\n",
        "    ribbon_width = int(0.3 * pixel_array.shape[1])\n",
        "    ribbon_position = pixel_array.shape[1] // 2 - ribbon_width // 2\n",
        "    pixel_array[:, ribbon_position:ribbon_position + ribbon_width] = 0\n",
        "    clahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(4, 4))\n",
        "    enhanced_image = clahe.apply(pixel_array)\n",
        "    alpha = 1.5\n",
        "    result_image = cv2.convertScaleAbs(enhanced_image, alpha=alpha, beta=0)\n",
        "    height, width = result_image.shape\n",
        "    upper_half = result_image[:height//3, :]\n",
        "    lower_half = result_image[height//3 * 2:, :]\n",
        "    if 'vertical' in filename or 'horizontal_and_vertical' in filename:\n",
        "      upper_half[:, :] = 0\n",
        "    else:\n",
        "      lower_half[:, :] = 0\n",
        "    image = Image.fromarray(result_image)\n",
        "    size = (224 ,224)\n",
        "    image = image.resize(size)\n",
        "    # plt.imshow(image, cmap=\"gray\")\n",
        "    # plt.axis('off')\n",
        "    # plt.show()\n",
        "    image = np.array(image)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "    image = preprocess_input(image)\n",
        "    images.append(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LOAD ALREADY FILTERED AND ROTATED DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOGk1jJGhgEy"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive/decayed_part/rotated_molar\"\n",
        "for filename in os.listdir(path):\n",
        "    ds = pydicom.read_file(os.path.join(path, filename))\n",
        "    if 'ndm' in filename:\n",
        "      labels.append(1)\n",
        "    else:\n",
        "      labels.append(2)\n",
        "    pixel_array = ds.pixel_array.astype(np.uint8)\n",
        "    clahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(4, 4))\n",
        "    enhanced_image = clahe.apply(pixel_array)\n",
        "    alpha = 1.5\n",
        "    result_image = cv2.convertScaleAbs(enhanced_image, alpha=alpha, beta=0)\n",
        "    image = Image.fromarray(result_image)\n",
        "    size = (224 ,224)\n",
        "    image = image.resize(size)\n",
        "    # plt.imshow(image, cmap=\"gray\")\n",
        "    # plt.axis('off')\n",
        "    # plt.show()\n",
        "    image = np.array(image)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "    image = preprocess_input(image)\n",
        "    images.append(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AUQTh3KEbFn",
        "outputId": "c14268d9-b13e-4acc-c15e-f2952a7f0564"
      },
      "outputs": [],
      "source": [
        "images = np.array(images)\n",
        "images = np.stack(images, axis=0)\n",
        "print(len(labels))\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IL9XuAn4JhfQ",
        "outputId": "8bd7c4fb-aa30-4012-8a07-956846bd07a1"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "label_encoder.fit(labels)\n",
        "\n",
        "labels = label_encoder.transform(labels)\n",
        "\n",
        "labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lc8skBrZJsGH",
        "outputId": "b1f2b415-538d-4f53-86d1-abe50ebdbebc"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(images, labels, test_size=0.18, random_state=23, stratify=labels)\n",
        "\n",
        "print(\"Train data shape:\", train_images.shape)\n",
        "print(\"Train labels shape:\", train_labels.shape)\n",
        "print(\"Validation data shape:\", val_images.shape)\n",
        "print(\"Validation labels shape:\", val_labels.shape)\n",
        "# print(\"Test data shape:\", test_images.shape)\n",
        "# print(\"Test labels shape:\", test_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IjqoET0JtVi",
        "outputId": "fe029fb7-02df-42c7-cba4-aae8305fbae1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.regularizers import l1_l2, l2, l1\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, AveragePooling2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=1, restore_best_weights=True)\n",
        "base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "model = keras.Sequential([\n",
        "        base_model,\n",
        "        # layers.BatchNormalization(),\n",
        "        # layers.AveragePooling2D(pool_size=(2, 2), padding='same'),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(512, activation='relu', kernel_regularizer=l2(0.1)),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(256, activation='relu', kernel_regularizer=l2(0.1)),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(1, activation='sigmoid')])\n",
        "# freeze layers\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = False\n",
        "# for layer in base_model.layers[-20:]:\n",
        "#   layer.trainable = True\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.0008), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(train_images, train_labels, batch_size=128, epochs=100, validation_data=(val_images, val_labels), callbacks=[early_stopping])\n",
        "model.save('/path/to/decayed_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "h9y2SrYBJ4WP",
        "outputId": "9b685b89-d02e-4335-8409-363982ba2211"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "train_accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "epochs = range(1, len(train_loss) + 1)\n",
        "# plt.plot(epochs, train_loss, 'b-', label='Training Loss')\n",
        "# plt.plot(epochs, val_loss, 'r-', label='Validation Loss')\n",
        "plt.plot(epochs, train_accuracy, 'b--', label='Training Accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'r--', label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Metrics')\n",
        "plt.title('Training and Validation Metrics')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "FINE TUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CX1Yv-QC6fyb",
        "outputId": "d6d6e061-0848-4677-86cc-30a1babdced6"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "# model = load_model('/path/to/decayed_model.h5')\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.000008), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(train_images, train_labels, batch_size=128, epochs=100, validation_data=(val_images, val_labels), callbacks=[early_stopping])\n",
        "model.save('/path/to/finetuned_decayed_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "fiHekVum6xcu",
        "outputId": "e75d9dc1-2848-43ac-ee68-a0ad67de066a"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "train_accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "epochs = range(1, len(train_loss) + 1)\n",
        "plt.plot(epochs, train_loss, 'b-', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'r-', label='Validation Loss')\n",
        "plt.plot(epochs, train_accuracy, 'b--', label='Training Accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'r--', label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Metrics')\n",
        "plt.title('Training and Validation Metrics')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GET YOUR MODEL'S RESULTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgIWdi1l6zJ6",
        "outputId": "22edcd77-a0d1-4055-94ea-75a966dcf096"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "# model = load_model('/content/drive/MyDrive/decayed_part/resnet50_ribbon_finetuned_decayed_premolar_model.h5')\n",
        "test_predictions = model.predict(val_images)\n",
        "predicted_labels = np.round(test_predictions).astype(int).flatten()\n",
        "true_labels = val_labels.flatten()\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "# Calculate precision\n",
        "precision = precision_score(true_labels, predicted_labels)\n",
        "print(\"Precision:\", precision)\n",
        "\n",
        "# Calculate recall\n",
        "recall = recall_score(true_labels, predicted_labels)\n",
        "print(\"Recall:\", recall)\n",
        "\n",
        "# Calculate F1-score\n",
        "f1 = f1_score(true_labels, predicted_labels)\n",
        "print(\"F1-score:\", f1)\n",
        "\n",
        "# Assuming you have predicted probabilities for binary classification\n",
        "predicted_probabilities = test_predictions[:, 0]  # Extract probabilities for positive class\n",
        "\n",
        "# Calculate AUC\n",
        "auc = roc_auc_score(true_labels, predicted_probabilities)\n",
        "print(\"AUC:\", auc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
