{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_Wsd-lFXOZT",
        "outputId": "99ff26a4-e85c-4841-968f-4f90ca23029a"
      },
      "outputs": [],
      "source": [
        "# if your are using DCM images\n",
        "!pip install pydicom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CONVERT DCM TO JPG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kChYU50W-1F"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pydicom\n",
        "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "# Define input and output folders\n",
        "input_folder_path = '/path/to/dcm'\n",
        "output_folder_path = '/path/to/jpg'\n",
        "# Create the output folder if it doesn't exist\n",
        "os.makedirs(output_folder_path, exist_ok=True)\n",
        "# Function to convert DICOM to RGB\n",
        "def dicom_to_rgb(dcm_path):\n",
        "  # Read DICOM file\n",
        "  dcm = pydicom.dcmread(dcm_path)\n",
        "  # Apply VOI LUT (if present)\n",
        "  image = apply_voi_lut(dcm.pixel_array, dcm)\n",
        "  # Convert to 8-bit unsigned integer\n",
        "  image = ((image / image.max()) * 255).astype(np.uint8)\n",
        "  # Stack the image to create an RGB image\n",
        "  rgb_image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "  return rgb_image\n",
        "# Iterate through DICOM files in the input folder\n",
        "for filename in os.listdir(input_folder_path):\n",
        "  if filename.endswith('.DCM'):\n",
        "    dcm_path = os.path.join(input_folder_path, filename)\n",
        "    # Convert DICOM to RGB\n",
        "    rgb_image = dicom_to_rgb(dcm_path)\n",
        "    # Save as JPG in the output folder\n",
        "    jpg_path = os.path.join(output_folder_path, os.path.splitext(filename)[0] + '.jpg')\n",
        "    cv2.imwrite(jpg_path, cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDweZUax3fUP"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "import six\n",
        "import time\n",
        "import glob\n",
        "from IPython.display import display\n",
        "\n",
        "from six import BytesIO\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "import tensorflow as tf\n",
        "from object_detection.utils import ops as utils_ops\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "\n",
        "%matplotlib inline\n",
        "def load_image_into_numpy_array(path):\n",
        "  \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "  Puts image into numpy array to feed into tensorflow graph.\n",
        "  Note that by convention we put it into a numpy array with shape\n",
        "  (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "  Args:\n",
        "    path: a file path (this can be local or on colossus)\n",
        "\n",
        "  Returns:\n",
        "    uint8 numpy array with shape (img_height, img_width, 3)\n",
        "  \"\"\"\n",
        "  img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "  image = Image.open(BytesIO(img_data))\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (im_height, im_width, 3)).astype(np.uint8)\n",
        "labelmap_path = '/path/to/label_map.pbtxt'\n",
        "category_index = label_map_util.create_category_index_from_labelmap(labelmap_path, use_display_name=True)\n",
        "output_directory = '/path/to/faster_rcnn_inference_graph'\n",
        "tf.keras.backend.clear_session()\n",
        "model = tf.saved_model.load(f'{output_directory}/saved_model')\n",
        "def run_inference_for_single_image(model, image):\n",
        "  image = np.asarray(image)\n",
        "  # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
        "  input_tensor = tf.convert_to_tensor(image)\n",
        "  # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
        "  input_tensor = input_tensor[tf.newaxis,...]\n",
        "\n",
        "  # Run inference\n",
        "  model_fn = model.signatures['serving_default']\n",
        "  output_dict = model_fn(input_tensor)\n",
        "\n",
        "  # All outputs are batches tensors.\n",
        "  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
        "  # We're only interested in the first num_detections.\n",
        "  num_detections = int(output_dict.pop('num_detections'))\n",
        "  output_dict = {key:value[0, :num_detections].numpy()\n",
        "                 for key,value in output_dict.items()}\n",
        "  output_dict['num_detections'] = num_detections\n",
        "\n",
        "  # detection_classes should be ints.\n",
        "  output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
        "\n",
        "  # Handle models with masks:\n",
        "  if 'detection_masks' in output_dict:\n",
        "    # Reframe the the bbox mask to the image size.\n",
        "    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "              output_dict['detection_masks'], output_dict['detection_boxes'],\n",
        "               image.shape[0], image.shape[1])\n",
        "    detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
        "                                       tf.uint8)\n",
        "    output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
        "\n",
        "  return output_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LOAD YOUR MODELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-NcrejO82yc"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from keras.models import load_model\n",
        "molar_premolar_model = load_model('/path/to/molar_premolar_model.h5')\n",
        "overlap_premolar_model = load_model('/path/to/overlap_premolar_model.h5')\n",
        "overlap_molar_model = load_model('/path/to/overlap_molar_model.h5')\n",
        "decayed_premolar_model = load_model('/path/to/decayed_premolar_model.h5')\n",
        "decayed_molar_model = load_model('/path/to/decayed_molar_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PATH YOUR BITEWING JPG IMAGES DIRECTORY TO PREDICT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "voSpWZR20BKa",
        "outputId": "6b1a6412-5d88-4efd-985f-ac184416eea3"
      },
      "outputs": [],
      "source": [
        "path = \"/path/to/jpg_images\"\n",
        "for filename in os.listdir(path):\n",
        "  for image_path in glob.glob(os.path.join(path, filename)):\n",
        "    image_np = load_image_into_numpy_array(image_path)\n",
        "    output_dict = run_inference_for_single_image(model, image_np)\n",
        "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "        image_np,\n",
        "        output_dict['detection_boxes'],\n",
        "        output_dict['detection_classes'],\n",
        "        output_dict['detection_scores'],\n",
        "        category_index,\n",
        "        instance_masks=output_dict.get('detection_masks_reframed', None),\n",
        "        use_normalized_coordinates=True,\n",
        "        line_thickness=1)\n",
        "    display(Image.fromarray(image_np))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2NVa4OodDbn",
        "outputId": "6e748c23-edb0-46f5-d2b0-2e93720c16c4"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import tensorflow as tf\n",
        "from object_detection.utils import ops as utils_ops\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob\n",
        "# Function to crop teeth based on bounding boxes\n",
        "direction = []\n",
        "def crop_teeth(image, boxes, classes, scores, category_index):\n",
        "  cropped_teeth = []\n",
        "  for i in range(len(boxes)):\n",
        "    if scores[i] > 0.5:\n",
        "      box = tuple(boxes[i].tolist())\n",
        "      ymin, xmin, ymax, xmax = box\n",
        "      if ymin < 0.2:\n",
        "        direction.append('up')\n",
        "      else:\n",
        "        direction.append('down')\n",
        "      ymin, xmin, ymax, xmax = int(ymin * image.shape[0]), int(xmin * image.shape[1]), int(ymax * image.shape[0]), int(xmax * image.shape[1]) # Crop the tooth region from the image\n",
        "      tooth_cropped = image[ymin:ymax, xmin:xmax]\n",
        "      newsize = (224, 224)\n",
        "      tooth_cropped = cv2.resize(tooth_cropped, newsize)\n",
        "      cropped_teeth.append(tooth_cropped)\n",
        "  return cropped_teeth\n",
        "c = -1\n",
        "path = \"/path/to/bitewing_jpg_images\"\n",
        "for filename in os.listdir(path):\n",
        "  image_np = load_image_into_numpy_array(os.path.join(path, filename))\n",
        "  output_dict = run_inference_for_single_image(model, image_np)\n",
        "  # Crop teeth based on bounding boxes\n",
        "  cropped_teeth = crop_teeth(image_np, output_dict['detection_boxes'], output_dict['detection_classes'], output_dict['detection_scores'], category_index)\n",
        "  # Now 'cropped_teeth' is a list containing cropped images of detected teeth\n",
        "  for i, tooth_cropped in enumerate(cropped_teeth):\n",
        "    x = tooth_cropped.copy()\n",
        "    plt.imshow(x)\n",
        "    plt.show()\n",
        "    c += 1\n",
        "    tooth_cropped = preprocess_input(tooth_cropped)\n",
        "    # tooth_cropped = np.array(tooth_cropped)\n",
        "    tooth_cropped = np.expand_dims(tooth_cropped, axis=0)\n",
        "    test_predictions = molar_premolar_model.predict(tooth_cropped)\n",
        "    predicted_labels = np.round(test_predictions).astype(int).flatten()\n",
        "    print('premolar' if predicted_labels == 0 else 'molar')\n",
        "    if predicted_labels == 1:\n",
        "      # tooth_cropped = cv2.resize(tooth_cropped[0], (134, 224))\n",
        "      # tooth_cropped = tooth_cropped.reshape(1, 224, 134, 3)\n",
        "      test_predictions = overlap_molar_model.predict(tooth_cropped)\n",
        "      predicted_labels = np.argmax(test_predictions, axis=1)\n",
        "      print('overlapped' if predicted_labels == 0 else 'non-overlapped' if predicted_labels==1 else 'useless')\n",
        "      if predicted_labels == 1:\n",
        "        x = x.astype(np.uint8)\n",
        "        gray_image = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n",
        "        clahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(4, 4))\n",
        "        enhanced_image = clahe.apply(gray_image)\n",
        "        alpha = 1.5\n",
        "        result_image = cv2.convertScaleAbs(enhanced_image, alpha=alpha, beta=0)\n",
        "        ribbon_width = int(0.3 * result_image.shape[1])\n",
        "        ribbon_position = result_image.shape[1] // 2 - ribbon_width // 2\n",
        "        result_image[:, ribbon_position:ribbon_position + ribbon_width] = 0\n",
        "        height, width = result_image.shape\n",
        "        upper_half = result_image[:height//3, :]\n",
        "        lower_half = result_image[height//3 * 2:, :]\n",
        "        if 'up' == direction[c]:\n",
        "          upper_half[:, :] = 0\n",
        "        else:\n",
        "          lower_half[:, :] = 0\n",
        "        image = cv2.cvtColor(result_image, cv2.COLOR_GRAY2RGB)\n",
        "        tooth_cropped = tf.keras.applications.resnet.preprocess_input(image)\n",
        "        # tooth_cropped = np.array(tooth_cropped)\n",
        "        tooth_cropped = np.expand_dims(tooth_cropped, axis=0)\n",
        "        tooth_cropped = cv2.resize(tooth_cropped[0], (224, 224))\n",
        "        tooth_cropped = tooth_cropped.reshape(1, 224, 224, 3)\n",
        "        test_predictions = decayed_molar_model.predict(tooth_cropped)\n",
        "        predicted_labels = np.round(test_predictions).astype(int).flatten()\n",
        "        print('non-decayed' if predicted_labels == 0 else 'decayed')\n",
        "    else:\n",
        "      tooth_cropped = cv2.resize(tooth_cropped[0], (134, 224))\n",
        "      tooth_cropped = tooth_cropped.reshape(1, 224, 134, 3)\n",
        "      test_predictions = overlap_premolar_model.predict(tooth_cropped)\n",
        "      predicted_labels = np.argmax(test_predictions, axis=1)\n",
        "      print('overlapped' if predicted_labels == 0 else 'non-overlapped' if predicted_labels==1 else 'useless')\n",
        "      if predicted_labels == 1:\n",
        "        x = x.astype(np.uint8)\n",
        "        gray_image = cv2.cvtColor(x, cv2.COLOR_BGR2GRAY)\n",
        "        clahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(4, 4))\n",
        "        enhanced_image = clahe.apply(gray_image)\n",
        "        alpha = 1.5\n",
        "        result_image = cv2.convertScaleAbs(enhanced_image, alpha=alpha, beta=0)\n",
        "        ribbon_width = int(0.3 * result_image.shape[1])\n",
        "        ribbon_position = result_image.shape[1] // 2 - ribbon_width // 2\n",
        "        result_image[:, ribbon_position:ribbon_position + ribbon_width] = 0\n",
        "        height, width = result_image.shape\n",
        "        upper_half = result_image[:height//3, :]\n",
        "        lower_half = result_image[height//3 * 2:, :]\n",
        "        if 'up' == direction[c]:\n",
        "          upper_half[:, :] = 0\n",
        "        else:\n",
        "          lower_half[:, :] = 0\n",
        "        print(direction[c-1])\n",
        "        image = cv2.cvtColor(result_image, cv2.COLOR_GRAY2RGB)\n",
        "        tooth_cropped = tf.keras.applications.resnet.preprocess_input(image)\n",
        "        # tooth_cropped = np.array(tooth_cropped)\n",
        "        tooth_cropped = np.expand_dims(tooth_cropped, axis=0)\n",
        "        tooth_cropped = cv2.resize(tooth_cropped[0], (134, 224))\n",
        "        tooth_cropped = tooth_cropped.reshape(1, 224, 134, 3)\n",
        "        test_predictions = decayed_premolar_model.predict(tooth_cropped)\n",
        "        predicted_labels = np.round(test_predictions).astype(int).flatten()\n",
        "        print('non-decayed' if predicted_labels == 0 else 'decayed')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.12.1 ('env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "71c90ae4103388b5842baf4730faf3518618fc2e94939623687e520799d7e1fe"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
